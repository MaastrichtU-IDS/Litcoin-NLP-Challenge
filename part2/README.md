
## Environment Details
The models were developed and trained using a Tesla V100-SXM2 with 32 GB Memory

## Installation
PART2: Make sure you installed all dependendices with
```bash
  pip3 install -r requirements.txt
```
## Runining models

1- Add all challenge files to './data' folder

2- Next, run the notebooks in JupyterLab in the following order:
```bash
  step1_generate_traindata.ipynb 
  step2_generate_traindata_novelty.ipynb 
  step3_generate_testdata.ipynb 
  step4_bert_classifier_biobert.ipynb 
  step5_bert_classifier_novelty.ipynb 
  step6_bert_predict-final.ipynb
```

**Step 1** -  Generate relation candidates by employing filtering;  if two different entities are included in the same sentence or the neighbouring sentences (the sentences on the right and left), this entity pair is considered as a candidate relation. We have also determined the valid entity pair types for the relation candidate. These entity types were generated by extracting the possible entity pairs observed in the training file for relations. For example,  ('ChemicalEntity', 'DiseaseOrPhenotypicFeature') is a valid entity-entity type, but ('DiseaseOrPhenotypicFeature', 'DiseaseOrPhenotypicFeature') is not a valid entity-entity type. Next, we have checked If these candidate relations overlap with the relations in the relation train set, then this candidate relation type is assigned to the same as the relation training set. Otherwise, these entity pairs are labeled as "No Relation"  (coded as "Negative').

----
**Step 2**- Generate training set to predict novelty column as similar to step 1. However, it uses only the entity-entity pairs that have existing types. We have not considered the unknown relations ("No Relation") for novelty. The labelled dataset consists of the text (sentence or the neighout sentence) and novelty flag. 

----
**Step 3**- Relation candidates are generated for the test set. The relation types are not assigned as relation types were not known beforehand.

----
**Step 4**-  We fine-tuned BioBERT (HuggingFace's BioBERT https://huggingface.co/dmis-lab/biobert-base-cased-v1.1) for the annotated dataset generated in step 1. 

----
**Step 5**-  A BER model (bert-base-uncased) is used to train a model on the generated dataset from step 2 to predict the novelty colum .

----
**Step 6**-  The predictions from both models (relation and novelty) were generated. We have excluded the predictions which have 'Negative' meaning no relations between given entities.

----


3- the submission file will be generated in 'data' folder with filename : 
```bash
  ./data/submission_xx.csv
```
